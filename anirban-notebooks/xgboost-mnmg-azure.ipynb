{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#8735fb; font-size:24pt\"> Multi Node Multi-GPU example on Azure using XGBoost and dask-cloudprovider </span>\n",
    "\n",
    "[Dask Cloud Provider](https://cloudprovider.dask.org/en/latest/) is a native cloud intergration library for Dask. It helps manage Dask clusters on different cloud platforms. In this notebook, we will look at how we can use this package to set-up an Azure cluster and run a multi-node, multi-GPU example with [RAPIDS](https://rapids.ai/). RAPIDS provides a suite of libraries to accelerate data science pipelines on the GPU entirely. This can be scaled to multiple nodes using Dask as we will see in this notebook. \n",
    "\n",
    "For the purposes of this demo, we will use a part of the NYC Taxi Dataset (only the files of 2014 calendar year will be used here). The goal is to predict the fare amount for a given trip given the times and coordinates of the taxi trip. We will download the data from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/), where the dataset is publicly hosted by Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step -1: Set up Azure credentials and cli. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, run the following commands in the terminal to setup Azure CLI\n",
    "```\n",
    "curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
    "az login\n",
    "```\n",
    "Then, follow the instructions on the prompt to finish setting up the account. If you are running the notebook from inside a Docker container, you can remove `sudo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:22pt\"> Step 0: Import necessary packages. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following and install some libraries at the beginning. \n",
    "# # If azureml is not present install azureml-core. \n",
    "# # Further opendatasets is in preview mode hence it is not available in the main sdk and needs to be installed separately.\n",
    "# # Installing azureml-opendatasets with for some weird reason installs an old version of Pandas 1.0.0, which we will upgrade to 1.2.4 to work with RAPIDS. \n",
    "# # We do the same for the numpy and scipy libraries that are also downgraded by azureml-opendatasets.\n",
    "# ! pip install \"dask-cloudprovider[azure]\"\n",
    "# ! pip install \"dask-cloudprovider[azure]\" --upgrade\n",
    "# ! pip install azureml-core\n",
    "# ! pip install azureml-opendatasets\n",
    "# ! pip install azureml-telemetry\n",
    "# ! pip install pandas==1.2.4 # reverting pandas to 1.2.4\n",
    "# ! pip install numpy==1.20.2 # reverting numpy to 1.20.2\n",
    "# ! pip install scipy==1.6.0 # reverting scipy to 1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait, get_worker\n",
    "from dask_cloudprovider.azure import AzureVMCluster\n",
    "import dask_cudf\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.metrics import mean_squared_error\n",
    "from cuml import ForestInference\n",
    "import cudf\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 1: Set up the Azure VM Cluster </span>\n",
    "\n",
    "We will now set up an [Azure VM Cluster](https://cloudprovider.dask.org/en/latest/azure.html) using `AzureVMCluster` from Dask Cloud Provider, which creates a cluster of VMs to run our workload. To do this, you will first need to set up a Resource Group, a Virtual Network and a Security Group on Azure. [Learn more about how you can set this up](https://cloudprovider.dask.org/en/latest/azure.html#resource-groups). Note that you can also set it up using the Azure portal directly via the GUI.\n",
    "\n",
    "Once you have set it up, you can now plug in the names of the entities you have created in the cell below. Finally note that we use the RAPIDS docker image to build the VM and use the `dask_cuda.CUDAWorker` to run within the VM. This will create the worker docker image with cuda capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a custom vm image with the newest kernel installed in it. You can use this guide: https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/virtual-machines/linux/tutorial-custom-images.md . Unless you use the VM with newest kernel, creating an Azure VM will install the newest kernel along with the NVIDIA drivers. Then the VM won't work without restarting the machine. Further, if you restart the machine, then the system would not work either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"logging.kubernetes\": \"info\",\n",
    "                 \"logging.distributed\": \"info\",\n",
    "                \"cloudprovider.azure.azurevm.vm_image\":{\"id\":<id of the custom image version>}})\n",
    "config = dask.config.get(\"cloudprovider.azure.azurevm\", {})\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"some location\"\n",
    "resource_group = \"some resource group\"\n",
    "vnet = \"some vnet\"\n",
    "security_group = \"some security group\"\n",
    "\n",
    "vm_size = \"Standard_NC12s_v3\"\n",
    "docker_image = \"rapidsai/rapidsai:cuda11.2-runtime-ubuntu18.04-py3.8\"\n",
    "worker_class = \"dask_cuda.CUDAWorker\"\n",
    "docker_args = '--shm-size=256m'\n",
    "worker_options = {'rmm-managed-memory':True}\n",
    "vm_image = {\"id\":<id of the custom image version>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step may take several minutes to start the VMs and download the docker images. Sit tight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AzureVMCluster.get_cloud_init(\n",
    "resource_group=resource_group,\n",
    "location = location,\n",
    "vnet=vnet,\n",
    "security_group=security_group,\n",
    "n_workers=0,\n",
    "vm_size=vm_size,\n",
    "docker_image=docker_image,\n",
    "docker_args=docker_args,\n",
    "auto_shutdown=False,\n",
    "security=True,\n",
    "vm_image=vm_image,\n",
    "worker_class=\"dask_cuda.CUDAWorker\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cluster = AzureVMCluster(\n",
    "    location=location,\n",
    "    resource_group=resource_group,\n",
    "    vnet=vnet,\n",
    "    security_group=security_group,\n",
    "    vm_size=vm_size,\n",
    "    docker_image=docker_image,\n",
    "    worker_class=worker_class,\n",
    "    n_workers=0,\n",
    "    security=True,\n",
    "    docker_args=docker_args,\n",
    "    worker_options=worker_options,\n",
    "    debug=True,\n",
    "    vm_image=vm_image\n",
    "#    env_vars = env_vars\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster) \n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_workers(client, n_workers, timeout=300):\n",
    "    import time\n",
    "    client.cluster.scale(n_workers)\n",
    "    m = len(client.has_what().keys())    \n",
    "    start = end = time.perf_counter_ns()\n",
    "    while ((m < n_workers) and (((end - start) / 1e9) < timeout) ):\n",
    "        time.sleep(5)\n",
    "        m = len(client.has_what().keys())\n",
    "        \n",
    "        end = time.perf_counter_ns()\n",
    "        \n",
    "    if (((end - start) / 1e9) >= timeout):\n",
    "        raise RuntimeError(f\"Failed to rescale cluster in {timeout} sec.\"\n",
    "              \"Try increasing timeout for very large containers, and verify available compute resources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scale_workers(client, 2, timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(scheduler_dict):\n",
    "    print(f\"All workers for scheduler id: {scheduler_dict['id']}, address: {scheduler_dict['address']}\")\n",
    "    for worker in scheduler_dict['workers']:\n",
    "        print(f\"Worker: {worker} , gpu_machines: {scheduler_dict['workers'][worker]['gpu']}\")\n",
    "\n",
    "pretty_print(client.scheduler_info()) # will show information on the len(CUDA_VISIBLE_DEVICES) partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 2: Data Setup, Cleanup and Enhancement </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.a: Set the path for downloading the data from Azureml Opendatasets </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()    \n",
    "start_date = parser.parse('2014-05-01') # lets start at 1st May 2014\n",
    "end_date = parser.parse('2014-05-31') # Lets stop at 31st May 2014\n",
    "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nyc_tlc_df.shape) # Apprx. 14 Million rows\n",
    "print(type(nyc_tlc_df))\n",
    "print(nyc_tlc_df.head()) \n",
    "# since we are going to send the data to the server lets use the first 10million rows\n",
    "nyc_tlc_df = nyc_tlc_df[:10000000]\n",
    "print(nyc_tlc_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data locally to see what we're dealing with. We will make use of the data from 2014 for the purposes of the demo. We see that there are columns for pickup and dropoff times, distance, along with latitude, longitude, etc. These are the information we'll use to estimate the trip fare amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.b: Data Cleanup, Enhancement and Persisting Scripts </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be cleaned up first. We remove some columns that we are not interested in. We also define the datatypes each of the columns need to be read as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add some new features to our dataframe via some custom functions, namely: \n",
    "1. Haversine distance: This is used for calculating the total trip distance.\n",
    "\n",
    "2. Day of the week: This can be useful information for determining the fare cost.\n",
    "\n",
    "`add_features` function combined the two to produce a new dataframe that has the added features.\n",
    "\n",
    "**NOTE**: We will also persist the test dataset in the workers. If the X_infer i.e. the test dataset is small enough, we can call compute() on it to bring the test dataset to the local machine and then perform predict on it. But in general, if the X_infer is large, it may not fit in the GPU(s) of the local machine. Moreover, moving around a large amount of data will also add to the prediction latency. Therefore it is better to persist the test dataset on the dask workers, and then call the predict functionality on the individual workers. Finally we collect the prediction results from the dask workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding features functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude_r, pickup_longitude_r, dropoff_latitude_r, dropoff_longitude_r, h_distance, radius):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude_r, pickup_longitude_r, dropoff_latitude_r, dropoff_longitude_r,)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        # radius = 6371 # Radius of earth in kilometers # currently passed as input arguments\n",
    "        \n",
    "        h_distance[i] = c * radius\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['tpepPickupDateTime'].dt.hour\n",
    "    df['year'] = df['tpepPickupDateTime'].dt.year\n",
    "    df['month'] = df['tpepPickupDateTime'].dt.month\n",
    "    df['day'] = df['tpepPickupDateTime'].dt.day\n",
    "    df['diff'] = (df['tpepPickupDateTime'] - df['tpepPickupDateTime']).dt.seconds #convert difference between pickup and dropoff into seconds\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['startLat']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['startLon']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['endLat']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['endLon']//.01*.01\n",
    "    \n",
    "    df = df.drop('tpepDropoffDateTime', axis=1)\n",
    "    df = df.drop('tpepPickupDateTime', axis =1)\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                   incols=['pickup_latitude_r', 'pickup_longitude_r', 'dropoff_latitude_r', 'dropoff_longitude_r'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict(radius=6371))\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for cleaning and persisting the data in the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_train_infer_split(client, df, response_dtype, response_id, infer_frac=1.0, random_state=42, shuffle=True):\n",
    "    workers = client.has_what().keys()\n",
    "    X, y = df.drop([response_id], axis=1), df[response_id].astype('float32')\n",
    "    infer_frac = max(0, min(infer_frac, 1.0))\n",
    "    X_train, X_infer, y_train, y_infer = train_test_split(X, y, shuffle=True, random_state=random_state, test_size=infer_frac)\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        X_train, y_train = client.persist(\n",
    "            collections=[X_train, y_train]) \n",
    "    \n",
    "    if (infer_frac != 1.0):\n",
    "        with dask.annotate(workers=set(workers)):\n",
    "            X_infer, y_infer = client.persist(\n",
    "                collections=[X_infer, y_infer])\n",
    "\n",
    "        wait([X_train, y_train, X_infer, y_infer])\n",
    "    else:\n",
    "        X_infer = X_train\n",
    "        y_infer = y_train\n",
    "\n",
    "        wait([X_train, y_train])\n",
    "    \n",
    "    return X_train, y_train, X_infer, y_infer\n",
    "\n",
    "\n",
    "def clean(df_part, must_haves):\n",
    "    \"\"\"\n",
    "    This function performs the various clean up tasks for the data\n",
    "    and returns the cleaned dataframe.\n",
    "    \"\"\"    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['tpepPickupDateTime', 'tpepDropoffDateTime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "            \n",
    "    return df_part\n",
    "\n",
    "    \n",
    "def taxi_data_loader(client, nyc_tlc_df, response_dtype=np.float32, infer_frac=1.0, random_state=0):\n",
    "    #create a list of columns & dtypes the df must have\n",
    "    must_haves = {\n",
    "     'tpepPickupDateTime': 'datetime64[ms]',\n",
    "     'tpepDropoffDateTime': 'datetime64[ms]',\n",
    "     'passengerCount': 'int32',\n",
    "     'tripDistance': 'float32',\n",
    "     'startLon': 'float32',\n",
    "     'startLat': 'float32',\n",
    "     'rateCodeId': 'int32',\n",
    "     'endLon': 'float32',\n",
    "     'endLat': 'float32',\n",
    "     'fareAmount': 'float32'\n",
    "    }\n",
    "\n",
    "    workers = client.has_what().keys()\n",
    "    response_id = 'fareAmount'\n",
    "\n",
    "    taxi_data = dask_cudf.from_cudf(cudf.from_pandas(nyc_tlc_df), npartitions=len(workers))\n",
    "    taxi_data = clean(taxi_data, must_haves)\n",
    "    taxi_data = taxi_data.map_partitions(add_features)\n",
    "    # Drop NaN values and convert to float32\n",
    "    taxi_data = taxi_data.dropna()\n",
    "    fields = ['passengerCount', 'tripDistance', 'startLon', 'startLat', 'rateCodeId',\n",
    "                 'endLon', 'endLat', 'fareAmount', 'diff', 'h_distance', 'day_of_week', 'is_weekend']\n",
    "    taxi_data = taxi_data.astype(\"float32\")\n",
    "    taxi_data = taxi_data[fields]\n",
    "    taxi_data = taxi_data.reset_index()\n",
    "    \n",
    "    return persist_train_infer_split(client, taxi_data, response_dtype, response_id, infer_frac, random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.c: Get the Split Data and persist across workers </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a bit of time since the data has to be distributed across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "tic = timer()\n",
    "X_train, y_train, X_infer, y_infer = taxi_data_loader(client, nyc_tlc_df, infer_frac=0.1, random_state=42)\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for ETL and persisting : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 3: Train a XGBoost Model </span>\n",
    "\n",
    "We are now ready to train a XGBoost model on the data and then predict the fare for each trip.\n",
    "\n",
    "Before we start the training process, let us take a quick look at the details of the GPUs in the worker pods that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(client.scheduler_info()) # will show information on the len(CUDA_VISIBLE_DEVICES) partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 3.a: Set training Parameters </span>\n",
    "\n",
    "In this training example, we will use RMSE as the evaluation metric. It is also worth noting that performing HPO will lead to a set of more optimal hyperparameters.\n",
    "\n",
    "Refer to the notebook [HPO-RAPIDS](./HPO-RAPIDS.ipynb) in this repository for how to perform HPO on Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.15,\n",
    "    'max_depth': 8,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 1,\n",
    "    'silent': True,\n",
    "    'verbose_eval': True,\n",
    "    'booster' : 'gbtree', # 'gblinear' not implemented in dask\n",
    "    'debug_synchronize': True,\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method':'gpu_hist',\n",
    "    'num_boost_rounds': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 3.b: Train XGBoost Model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is already persisted in the dask workers in the Kubernetes Cluster, the next steps should not take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "tic = timer()\n",
    "xgboost_output = xgb.dask.train(client, params,data_train, \n",
    "                                    num_boost_round=params['num_boost_rounds'])\n",
    "xgb_cuda_model = xgboost_output['booster']\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 3.c: Save the trained model to disk locally </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'trained-model_nyctaxi.xgb'\n",
    "xgb_cuda_model.save_model(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 4: Predict & Score using vanilla XGBoost Predict </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the predict and inplace_predict methods provided by the xgboost.dask library, out of the box. Later we will also use [Forest Inference Library (FIL)](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=forestinference#cuml.ForestInference) to perform prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y_test = y_infer.compute()\n",
    "wait(_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = xgb.dask.DaskDMatrix(client, X_infer)\n",
    "tic = timer()\n",
    "y_pred = xgb.dask.predict(client, xgb_cuda_model, d_test)\n",
    "y_pred= y_pred.compute()\n",
    "wait(y_pred)\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for xgb.dask.predict : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference with the inplace predict method of dask XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "y_pred = xgb.dask.inplace_predict(client, xgb_cuda_model, X_infer)\n",
    "y_pred = y_pred.compute()\n",
    "wait(y_pred)\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for inplace inference : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "print(\"Calculating MSE\")\n",
    "score = mean_squared_error(y_pred, _y_test)\n",
    "print(\"Workflow Complete - RMSE: \", np.sqrt(score))\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 5: Predict & Score using FIL or Forest Inference Library </span>\n",
    "\n",
    "[Forest Inference Library (FIL)](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=forestinference#cuml.ForestInference) provides GPU accelerated inference capabilities for tree models. We will import the FIL functionality from cuML library.\n",
    "\n",
    "It accepts a trained tree model in a treelite format (currently LightGBM, XGBoost and SKLearn GBDT and random forest models are supported). In general, using FIL allows for faster inference while using a large number of workers, and the latency benefits are more pronounced as the size of the dataset grows large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import ForestInference\n",
    "from dask.distributed import get_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here if the `X_test` is small enough we can call `compute()` on it. But in general, if the `X_test` is quite large, it is better to persist it on the different workers and then call the predict on the individual workers separately. We we show both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 5.a: Predict using `compute` on a single worker in case the test dataset is small. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "X_test_computed = X_infer.compute()\n",
    "wait(X_test_computed)\n",
    "loaded_model = ForestInference.load(model_filename, model_type='xgboost')\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "fil_pred = loaded_model.predict(X_test_computed)\n",
    "print(\"Final - RMSE: \", np.sqrt(score))\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic=timer()\n",
    "score = mean_squared_error(fil_pred, _y_test)\n",
    "print(\"Final - RMSE: \", np.sqrt(score))\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the RMSE results with that obtained without using FIL. You would see they are approximately same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 5.b: Predict using `persist` on multiple workers in case the test dataset is huge. </span>\n",
    "\n",
    "As noted in Step 2.b, in case the test dataset is huge, it makes sense to call predict individually on the dask workers instead of bringing the entire test dataset to the local machine.\n",
    "\n",
    "To perform prediction individually on the dask workers, each dask worker needs to load the XGB model using FIL. However, the dask workers are remote and do not have access to the locally saved model. Hence we need to send the locally saved XGB model to the dask workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = client.has_what().keys()\n",
    "print(workers)\n",
    "n_workers = len(workers)\n",
    "n_partitions = n_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipFile(zipname):\n",
    "    worker = get_worker()\n",
    "    import zipfile\n",
    "    import os\n",
    "    with zipfile.ZipFile(os.path.join(worker.local_directory, zipname)) as zf:\n",
    "        zf.extractall(worker.local_directory)\n",
    "\n",
    "def checkOrMakeLocalDir():\n",
    "    worker = get_worker()\n",
    "    import os\n",
    "    if not os.path.exists(worker.local_directory):\n",
    "        os.makedirs(worker.local_directory)\n",
    "    \n",
    "def workerModelInit(model_file):   \n",
    "    # this function will run in each worker and initialize the worker \n",
    "    import os\n",
    "    worker = get_worker()\n",
    "    worker.data[\"fil_model\"] = ForestInference.load(filename=os.path.join(worker.local_directory, model_file),model_type='xgboost')\n",
    "    \n",
    "def predict(input_df):\n",
    "    # this function will run in each worker and predict \n",
    "    worker = get_worker()\n",
    "    return worker.data[\"fil_model\"].predict(input_df)\n",
    "\n",
    "def persistModelinWorkers(client, zip_file_name, model_file_name):\n",
    "    import zipfile\n",
    "    zf = zipfile.ZipFile(zip_file_name, mode='w')\n",
    "    zf.write(f\"./{model_file_name}\")\n",
    "    zf.close()\n",
    "    # check to see if local directory present in workers\n",
    "    # if not present make it\n",
    "    fut = client.run(checkOrMakeLocalDir)\n",
    "    wait(fut)\n",
    "    # upload the zip file in workers\n",
    "    fut = client.upload_file(f\"./{zip_file_name}\")\n",
    "    wait(fut)\n",
    "    # unzip file in the workers\n",
    "    fut = client.run(unzipFile, zip_file_name)\n",
    "    wait(fut)\n",
    "    # load model using FIL in workers\n",
    "    fut = client.run(workerModelInit, model_file_name)\n",
    "    wait(fut)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the local model in the remote dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "persistModelinWorkers(client, \"zipfile_write.zip\", \"trained-model_nyctaxi.xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference with distributed predict with FIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "predictions = X_infer.map_partitions(predict, meta=\"float\") # this is like MPI reduce\n",
    "y_pred = predictions.compute()\n",
    "wait(y_pred)\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_csv = X_infer.iloc[:,0].shape[0].compute()\n",
    "print(f\"It took {toc-tic} seconds to predict on {rows_csv} rows using FIL distributedly on each worker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "score = mean_squared_error(y_pred, _y_test)\n",
    "toc = timer()\n",
    "print(\"Final - RMSE: \", np.sqrt(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 6: Clean up </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://distributed.dask.org/en/latest/limitations.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
