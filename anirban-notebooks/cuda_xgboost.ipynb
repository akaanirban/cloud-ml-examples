{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:24pt\"> **Multi-GPU example on Azure using dask-cuda** </span>\n",
    "\n",
    "[Dask Cuda](https://dask-cuda.readthedocs.io/en/latest/) is a library which extends Dask's distributed single machine [LocalCluster](https://docs.dask.org/en/latest/setup/single-distributed.html#localcluster) for use in workloads that can leverage multiple GPUs.  \n",
    "In this notebook, we explore how we can leverage multiple GPUs in a single node using XGboost. \n",
    "We will further explore how we can use ForestInference library of cuml to do inference. \n",
    "\n",
    "For the purposes of this demo, we will use the a part of the NYC Taxi Dataset (only the files of 2018 calendar year will be used here). The goal is to predict the fare amount for a given trip given the times and coordinates of the taxi trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:22pt\"> **Step 0: Import Stuff** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Installs a bunch of stuff as dependencies including pyspark ! screwing up the environment. Need to look for possible alternative sources for the data. Otherwise need to run this in a docker container. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If azureml is not present install azureml-core. \n",
    "# # Further opendatasets is in preview mode hence it is not available in the main sdk and needs to be installed separately.\n",
    "# # Installing azureml-opendatasets with for some weird reason install Pandas 1.0.0 which is not what we need. So we install back \n",
    "# # pandas 1.2.4 after we are done.\n",
    "# ! pip install azureml-core\n",
    "# ! pip install azureml-opendatasets\n",
    "# ! pip install azureml-telemetry\n",
    "# ! pip install pandas==1.2.4 # reverting pandas to 1.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait\n",
    "import dask_cudf\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.metrics import mean_squared_error\n",
    "from cuml import ForestInference\n",
    "import cudf\n",
    "import xgboost as xgb\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:22pt\"> **Step 1: Set up the Local CUDA Cluster** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of partitions to 8 for Dask. Depending on how many GPUs you have, you can set the number of partitions. \n",
    "# right now someone is doing something on CUDA 0\n",
    "CUDA_VISIBLE_DEVICES = [3,4,5,6,7]\n",
    "npartitions = 5 #8\n",
    "clust = LocalCUDACluster(host=\"\", \n",
    "                         CUDA_VISIBLE_DEVICES=CUDA_VISIBLE_DEVICES,\n",
    "                         n_workers=npartitions,\n",
    "                         scheduler_port = 8786)\n",
    "client = Client(clust)\n",
    "clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(scheduler_dict):\n",
    "    print(f\"All workers for scheduler id: {scheduler_dict['id']}, address: {scheduler_dict['address']}\")\n",
    "    for worker in scheduler_dict['workers']:\n",
    "        print(f\"Worker: {worker} , gpu_machines: {scheduler_dict['workers'][worker]['gpu']}\")\n",
    "\n",
    "pretty_print(client.scheduler_info()) # will show information on the len(CUDA_VISIBLE_DEVICES) partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:22pt\"> **Step 2: Data Setup, Cleanup and Enhancement** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.a: Download the Data from Azureml Opendatasets </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()    \n",
    "start_date = parser.parse('2018-05-01') # lets start at 1st May 2018\n",
    "end_date = parser.parse('2018-06-06') # Lets stop at 6th June 2018\n",
    "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nyc_tlc_df.shape) # Apprx. 10 Million rows\n",
    "print(type(nyc_tlc_df))\n",
    "print(nyc_tlc_df.head()) # does not work without pandas 1.0.0 which gets installed with azure-opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask_cudf.from_cudf(cudf.from_pandas(nyc_tlc_df), npartitions=npartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data locally to see what we're dealing with. We will make use of the data from 2018 for the purposes of the demo. We see that there are columns for pickup and dropoff times, distance, along with latitude, longitude, etc. These are the information we'll use to estimate the trip fare amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.b: Data Cleanup Scripts </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be cleaned up before it can be used in a meaningful way. We first perform a renaming of some columns to a cleaner name (for instance, some of the years have `tpep_ropoff_datetime` instead of `dropfoff_datetime`). We also define the datatypes each of the columns need to be read as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    " 'tpepPickupDateTime': 'datetime64[ms]',\n",
    " 'tpepDropoffDateTime': 'datetime64[ms]',\n",
    " 'passengerCount': 'int32',\n",
    " 'tripDistance': 'float32',\n",
    " 'startLon': 'float32',\n",
    " 'startLat': 'float32',\n",
    " 'rateCodeId': 'int32',\n",
    " 'endLon': 'float32',\n",
    " 'endLat': 'float32',\n",
    " 'fareAmount': 'float32'\n",
    "}\n",
    "\n",
    "def clean(df_part, must_haves):\n",
    "    \"\"\"\n",
    "    This function performs the various clean up tasks for the data\n",
    "    and returns the cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['tpepPickupDateTime', 'tpepDropoffDateTime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.c: Data Enhancements, Add interesting Features </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add new features by making use of \"uder defined functions\" on the dataframe. We'll make use of [apply_rows](https://docs.rapids.ai/api/cudf/stable/api.html#cudf.core.dataframe.DataFrame.apply_rows), which is similar to Pandas' apply funciton. `apply_rows` operation is [JIT compiled by numba](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) into GPU kernels. \n",
    "\n",
    "The kernels we define are - \n",
    "1. Haversine distance: This is used for calculating the total trip distance.\n",
    "\n",
    "2. Day of the week: This can be useful information for determining the fare cost.\n",
    "\n",
    "`add_features` function combined the two to produce a new dataframe that has the added features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "def haversine_distance_kernel(startLat, startLon, endLat, endLon, h_distance, radius):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(startLat, startLon, endLat, endLon,)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        # radius = 6371 # Radius of earth in kilometers # currently passed as input arguments\n",
    "        \n",
    "        h_distance[i] = c * radius\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['tpepPickupDateTime'].dt.hour\n",
    "    df['year'] = df['tpepPickupDateTime'].dt.year\n",
    "    df['month'] = df['tpepPickupDateTime'].dt.month\n",
    "    df['day'] = df['tpepPickupDateTime'].dt.day\n",
    "    df['diff'] = (df['tpepDropoffDateTime'] - df['tpepPickupDateTime']).dt.seconds #convert difference between pickup and dropoff into seconds\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['startLat']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['startLon']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['endLat']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['endLon']//.01*.01\n",
    "    \n",
    "    df = df.drop('tpepDropoffDateTime', axis=1)\n",
    "    df = df.drop('tpepPickupDateTime', axis =1)\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                   incols=['startLat', 'startLon', 'endLat', 'endLon'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict(radius=6371))\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df` is a dask dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the dataframe to clean up the outliers \n",
    "df = clean(df, must_haves)\n",
    "# Add new features to all the partitions of the dataframe\n",
    "taxi_df = df.map_partitions(add_features)\n",
    "# Drop NaN values and convert to float32\n",
    "taxi_df = taxi_df.dropna()\n",
    "taxi_df = taxi_df.astype(\"float32\")\n",
    "\n",
    "print(type(taxi_df)) # still delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.d: Split Data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = taxi_df.drop([\"fareAmount\"], axis=1), taxi_df[\"fareAmount\"].astype('float32')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.e: Persist Data across workers </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = client.has_what().keys()\n",
    "X_train, X_test, y_train, y_test = dask_utils.persist_across_workers(client,\n",
    "                                                       [X_train, X_test, y_train, y_test],\n",
    "                                                       workers=workers)\n",
    "wait([X_train, X_test, y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.visualize() # has len(CUDA_VISIBLE_DEVICES) partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:22pt\"> **Step 3: Train a XGBoost Model** </span>\n",
    "\n",
    "We are now ready to fit a XGBoost model on the data to predict the fare for the trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always good to check what is the condition of the GPUs you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 3.a: Set training Parameters </span>\n",
    "\n",
    "We will use the eval metrix RMSE for this problem. Note that for better performance we should perform HPO ideally to get the best parameters. \n",
    "\n",
    "Refer to the notebooks in the repository for how to perform automated HPO [using RayTune](https://github.com/rapidsai/cloud-ml-examples/blob/main/ray/notebooks/Ray_RAPIDS_HPO.ipynb) and [using Optuna](https://github.com/rapidsai/cloud-ml-examples/blob/main/optuna/notebooks/optuna_rapids.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.15,\n",
    "    'max_depth': 11,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 1,\n",
    "    'silent': True,\n",
    "    'verbose_eval': True,\n",
    "    'booster' : 'gbtree', # 'gblinear' not implemented in dask\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method':'gpu_hist',\n",
    "    'num_boost_rounds': 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 3.b: Train XGBoost Model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "data_train = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "xgboost_output = xgb.dask.train(client, params,data_train, \n",
    "                                    num_boost_round=params['num_boost_rounds'])\n",
    "xgb_cuda_model = xgboost_output['booster']\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 3.c: Save the Model to disk </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'trained-model_nyctaxi.xgb'\n",
    "xgb_cuda_model.save_model(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:22pt\"> **Step 4: Predict & Score using vanilla XGBoost Predict** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "d_test = xgb.dask.DaskDMatrix(client, X_test)\n",
    "y_pred = xgb.dask.predict(client, xgb_cuda_model, d_test)\n",
    "_y_pred, _y_test = y_pred.compute(), y_test.compute()\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "print(\"Calculating MSE\")\n",
    "score = mean_squared_error(_y_pred, _y_test)\n",
    "print(\"Workflow Complete - RMSE: \", np.sqrt(score))\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:22pt\"> **Step 5: Predict & Score using FIL or Forest Inference Library** </span>\n",
    "\n",
    "[ForestInference](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=forestinference#cuml.ForestInference) provides GPU accelerated inference capabilities for tree models. \n",
    "It accepts a **trained** tree model in a treelite format (currently LightGBM, XGBoost and SKLearn GBDT and random forest models\n",
    "are supported). \n",
    "However, you cannot use it to train anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import ForestInference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here if the `X_test` is small enough we can call `compute()` on it. But in general, if the `X_test` is quite large, it is better to persist it on the different workers and then call the predict on the individual workers separately. We we show both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 5.a: Predict using `compute` on a single worker in case the test dataset is small. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "X_test_computed = X_test.compute()\n",
    "loaded_model = ForestInference.load(model_filename, model_type='xgboost')\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "fil_pred = loaded_model.predict(X_test_computed)\n",
    "print(\"Final - RMSE: \", np.sqrt(score))\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic=timer()\n",
    "score = mean_squared_error(fil_pred, _y_test)\n",
    "print(\"Final - RMSE: \", np.sqrt(score))\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the RMSE results with that obtained without using FIL. You would see they are approximately same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 5.b: Predict using `persist` on multiple workers in case the test dataset is huge. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import get_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = client.has_what().keys()\n",
    "print(workers)\n",
    "n_workers = len(workers)\n",
    "n_partitions = n_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_model_init(model_file):   \n",
    "    # this function will run in each worker and initialize the worker \n",
    "    worker = get_worker()\n",
    "    worker.data[\"fil_model\"] = ForestInference.load(filename=model_file,model_type='xgboost')\n",
    "    \n",
    "def predict(input_df):\n",
    "    # this function will run in each worker and predict \n",
    "    worker = get_worker()\n",
    "    return worker.data[\"fil_model\"].predict(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist the `X_test` in the workers if not already persisted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = X_test.persist()\n",
    "# wait(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic= timer()\n",
    "client.run(worker_model_init, model_filename)\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "predictions = X_test.map_partitions(predict, meta=\"float\") # this is like MPI reduce\n",
    "y_pred = predictions.compute()\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "score = mean_squared_error(y_pred, _y_test)\n",
    "toc = timer()\n",
    "print(\"Final - RMSE: \", np.sqrt(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:22pt\"> **Step 6: Clean up** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "clust.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
