{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated SHAP values with XGBoost 1.3 on AWS\n",
    "\n",
    "With the release of XGBoost 1.3 comes an exciting new feature for model interpretability — GPU accelerated SHAP values. SHAP values are a technique for local explainability of model predictions. That is, they give you the ability to examine the impact of various features on model output in a principled way. SHAP at its core describes the average impact from adding a feature to a model, but does it in a way that attempts to account for all possible subsets of the other features as well. See [Lundberg et al.'s “From Local Explanations to Global Understanding with Explainable AI for Trees”](https://www.nature.com/articles/s42256-019-0138-9) for in-depth reading on SHAP values as applied to decision tree ensembles.\n",
    "\n",
    "In this notebook, we provide an example of training an XGBoost model with AWS SageMaker's [XGBoost estimator](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html), and then use SHAP values to identify key features and feature interactions in our dataset. SHAP values have been available in XGBoost for several versions already, but 1.3 brings GPU acceleration, reducing computation time by up to 20x for SHAP values and 340x for SHAP interaction values. This is powered under the hood by RAPIDS GPUTreeShap, which offers portable CUDA C++ implementations of SHAP algorithms for decision tree models. This algorithm is described in detail in [GPUTreeShap: Fast Parallel Tree Interpretability\n",
    "](https://arxiv.org/abs/2010.13972). \n",
    "\n",
    "We will be using the California housing dataset. This is a famous dataset of house prices and attributes in California from the 1990 Census, available via [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html).\n",
    "\n",
    "## 0. Preliminary Setup\n",
    "\n",
    "This notebook was tested in an Amazon [SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html) notebook, on a ml.t3.medium instance with Python 3 (Data Science) kernel. As a preliminary step, we first ensure that the latest version of SageMaker is installed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "1. The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "2. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regex with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-xgboost-inference-script-mode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training the XGBoost model\n",
    "\n",
    "SageMaker can now run an XGboost script using the [XGBoost estimator](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. In this notebook, we use the training script **train.py**.\n",
    "\n",
    "After setting training parameters, we kick off training. \n",
    "\n",
    "To run our training script on SageMaker, we construct a `sagemaker.xgboost.estimator.XGBoost` estimator, which accepts several constructor arguments:\n",
    "\n",
    "* __entry_point__: The path to the Python script SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __framework_version__: SageMaker XGBoost version you want to use for executing your model training code, e.g., `0.90-1`, `0.90-2`, `1.0-1`, or `1.3-1`. We must use `1.3-1` for GPU accelerated SHAP values.\n",
    "* __train_instance_type__ *(optional)*: The type of SageMaker instances for training. __Note__: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker.\n",
    "* __hyperparameters__ *(optional)*: A dictionary passed to the train function as hyperparameters. For the XGBoost estimator, the list of possible hyperparameters can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "job_name = \"DEMO-xgboost-inference-script-mode-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"6\",\n",
    "    \"eta\": \"0.3\",\n",
    "    \"gamma\": \"0\",\n",
    "    \"min_child_weight\": \"1\",\n",
    "    \"subsample\": \"1\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"500\",\n",
    "    \"verbosity\": \"1\",\n",
    "    \"sklearn_dataset\": \"sklearn.datasets.fetch_california_housing()\",\n",
    "    # using default \"tree_method\": \"gpu_hist\" and \"predictor\": \"gpu_predictor\"\n",
    "    # \"tree_method\": \"hist\", \"predictor\": \"cpu_predictor\",  # for CPU version\n",
    "}\n",
    "\n",
    "instance_type = \"ml.g4dn.xlarge\"  # \"ml.c5.xlarge\"\n",
    "\n",
    "xgb_script_mode_estimator = XGBoost(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    framework_version=\"1.3-1\",\n",
    "    output_path=\"s3://{}/{}/{}/output\".format(bucket, prefix, job_name),\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Since the estimator requires a valid file type but we are specifying a sklearn_dataset, \n",
    "we pass in a path to a tiny csv file which will not be used.\n",
    "\"\"\"\n",
    "content_type = \"text/csv\"  # MIME type\n",
    "train_input = TrainingInput(\n",
    "    \"s3://sagemaker-rapids-hpo-us-east-1/dummy_data.csv\", content_type=content_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Train XGBoost Estimator on California Housing Data\n",
    "\n",
    "Training is as simple as calling `fit` on the Estimator. This will start a SageMaker Training job that will download the data, invoke the entry point code (in the provided script file), and save any model artifacts that the script creates. \n",
    "\n",
    "Note in the cell above that we specify \"sklearn_dataset\": \"sklearn.datasets.fetch_california_housing()\" as a parameter in order to use the California housing dataset from scikit-learn. However, calling `fit` on the Estimator requires a valid filepath, but we do not want to have to download the scikit-learn data. Thus as a quick-fix, we refer to a small (only a couple of bytes large) existing dummy CSV file, which is immediately discarded by `train.py` upon receiving a valid \"sklearn_dataset\" input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_script_mode_estimator.fit({\"train\": train_input}, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploying the XGBoost endpoint\n",
    "\n",
    "After training, we can host the newly created model in SageMaker, and create an Amazon SageMaker endpoint – a hosted and managed prediction service that we can use to perform inference. If you call `deploy` after you call `fit` on an XGBoost estimator, it will create a SageMaker endpoint using the training script (i.e., `entry_point`). \n",
    "\n",
    "You can optionally specify other functions to customize the behavior of deserialization of the input request (`input_fn()`), serialization of the predictions (`output_fn()`), and how predictions are made (`predict_fn()`). First `input_fn()` is called, then its output is fed into `predict_fn()`, and finally `output_fn()` returns the predictions. Rather than defining three separate functions, you can also combine them all into a function `transform_fn()`, which is what we did for this example in **inference.py**. If any of these functions are not specified, the endpoint will use the default functions in the SageMaker XGBoost container. See the [SageMaker Python SDK documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#sagemaker-xgboost-model-server) for details.\n",
    "\n",
    "In this notebook, we will run a separate inference script and customize the endpoint to return [SHAP](https://github.com/slundberg/shap) values and interactions in addition to predictions. The inference script that we will run in this notebook is provided as the accompanying file `inference.py`. \n",
    "\n",
    "### 2.1 Deploy to an endpoint\n",
    "\n",
    "Since the inference script is separate from the training script, here we use `XGBoostModel` to create a model from s3 artifacts and specify `inference.py` as the `entry_point`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "\n",
    "model_data = xgb_script_mode_estimator.model_data\n",
    "print(model_data)\n",
    "\n",
    "xgb_inference_model = XGBoostModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.3-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = xgb_inference_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=None, deserializer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Make and plot predictions\n",
    "\n",
    "Now that we have fetched the dataset and trained an XGBoost regression model with 500 trees (using GPU acceleration), we can generate predictions on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = predictor.predict(\"sklearn.datasets.fetch_california_housing()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the output is a string, we define a method to clean it up and re-cast it as a NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def clean_array(arr, three_dim=False):\n",
    "    cleaned_list = []\n",
    "    arr_count = 0\n",
    "    for num in arr:\n",
    "        if '[' in num:\n",
    "            arr_count += 1\n",
    "            num = num.replace('[', '')\n",
    "            cleaned_list.append(float(num))\n",
    "        elif ']' in num:\n",
    "            num = num.replace(']', '')\n",
    "            cleaned_list.append(float(num))\n",
    "        else: \n",
    "            cleaned_list.append(float(num))\n",
    "            \n",
    "    array = np.array(cleaned_list, dtype='float32')\n",
    "    if three_dim:  # shap_interactions will be 3D\n",
    "        y = int( len(array) / arr_count )\n",
    "        x = int( arr_count / y )\n",
    "        array = array.reshape(x, y, y)\n",
    "    elif(arr_count > 1):\n",
    "        y = int( len(array) / arr_count )\n",
    "        array = array.reshape(arr_count, y)\n",
    "    return array\n",
    "    \n",
    "    \n",
    "predictions = clean_array(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's plot the distribution of predictions on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(predictions, bins=40, density=True)\n",
    "plt.title(\"House price prediction distribution\")\n",
    "plt.xlabel(\"$100k\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generate and plot SHAP values\n",
    "\n",
    "Here we generate the SHAP values of the training set. The time it took to compute all the SHAP values was: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "shap_values = predictor.predict(\"sklearn.datasets.fetch_california_housing(), pred_contribs\")\n",
    "print(\"SHAP time\", time.time() - start)\n",
    "\n",
    "shap_values = clean_array(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shap_values` now contains a matrix where each row is a training instance from X and the columns contain the feature attributions (i.e. the amount that each feature contributed to the prediction). The last column in the output shap_values contains the ‘bias’ or the expected output of the model if no features were used. Each row always adds up exactly to the model prediction — this is a unique advantage of SHAP values compared to other model explanation techniques.\n",
    "\n",
    "Model predictions can be inspected individually using this output, or we can aggregate the SHAP values to gain insight into global feature importance. Here we take the mean absolute contribution of each feature and plot their magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(feature_names, shap_values):\n",
    " # Get the mean absolute contribution for each feature\n",
    " aggregate = np.mean(np.abs(shap_values[:, 0:-1]), axis=0)\n",
    " # sort by magnitude\n",
    " z = [(x, y) for y, x in sorted(zip(aggregate, feature_names), reverse=True)]\n",
    " z = list(zip(*z))\n",
    " plt.bar(z[0], z[1])\n",
    " plt.xticks(rotation=90)\n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    "\n",
    "feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\",\n",
    "                 \"Latitude\", \"Longitude\"]\n",
    "plot_feature_importance(feature_names, shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows that latitude, longitude, and median income are the most relevant features to our XGBoost model. This chart shows first-order feature relevance, however, we can go deeper and search for second-order effects, or interactions between pairs of features. In the past, it was extremely expensive to search for second-order effects. \n",
    "\n",
    "### 2.4 Compute and plot SHAP interactions\n",
    "\n",
    "Now with GPUTreeShap we can compute these interaction effects in a matter of seconds, even for large datasets with many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "shap_interactions = predictor.predict(\"sklearn.datasets.fetch_california_housing(), pred_interactions\")\n",
    "print(\"SHAP interactions time\", time.time() - start)\n",
    "\n",
    "shap_interactions = clean_array(shap_interactions[0], three_dim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively small dataset with only 8x8 possible feature interactions. For larger datasets, as shown in our paper, GPUTreeShap can reduce feature interaction computations from days to a matter of minutes.\n",
    "\n",
    "The output `shap_interactions` contains a symmetric matrix of interaction terms for each row, where the element-wise sum evaluates to the model prediction. The diagonal terms represent the main effects for each feature or the impact of that feature excluding second-order interactions.\n",
    "\n",
    "As before we can aggregate interactions to examine the most significant effects over the training set. This time we plot only the top-k effects due to a large number of possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_k_interactions(feature_names, shap_interactions, k):\n",
    "    # Get the mean absolute contribution for each feature interaction\n",
    "    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n",
    "    interactions = []\n",
    "    for i in range(aggregate_interactions.shape[0]):\n",
    "        for j in range(aggregate_interactions.shape[1]):\n",
    "            if j < i:\n",
    "                interactions.append(\n",
    "                    (feature_names[i] + \"-\" + feature_names[j], aggregate_interactions[i][j] * 2))\n",
    "    # sort by magnitude\n",
    "    interactions.sort(key=lambda x: x[1], reverse=True)\n",
    "    interaction_features, interaction_values = map(tuple, zip(*interactions))\n",
    "    plt.bar(interaction_features[:k], interaction_values[:k])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_top_k_interactions(feature_names, shap_interactions, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the SHAP algorithm has identified Latitude-Longitude as the strongest interaction effect by a large margin. While this outcome is somewhat intuitive for the task of house price prediction, given our a priori knowledge of the relatedness of latitude and longitude, the SHAP interactions are useful in algorithmically verifying if the model is behaving according to our human intuition. In cases where the meaning or relationships between features is unclear, it can be used to search for interactions that are not immediately obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Delete the Endpoint\n",
    "\n",
    "If you're done with this exercise, please run the `delete_endpoint` line in the cell below.  This will remove the hosted endpoint and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. References\n",
    "\n",
    "- [GPU-Accelerated SHAP values with XGBoost 1.3 and RAPIDS](https://medium.com/rapids-ai/gpu-accelerated-shap-values-with-xgboost-1-3-and-rapids-587fad6822)\n",
    "\n",
    "- [SageMaker XGBoost Abalone example](https://github.com/aws/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/xgboost_abalone)\n",
    "\n",
    "- [SageMaker XGBoost docs](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/index.html)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
