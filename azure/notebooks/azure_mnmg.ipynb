{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-node multi-GPU example on Azure using dask-cloudprovider\n",
    "\n",
    "[Dask Cloud Provider](https://cloudprovider.dask.org/en/latest/) is a native cloud intergration for dask. It helps manage Dask clusters on different cloud platforms. In this notebook, we will look at how we can use the package to set-up a Azure cluster and run a multi-node, multi-GPU example with [RAPIDS](https://rapids.ai/). RAPIDS provides a suite of libraries to accelerate data science pipelines on the GPU entirely. This can be scaled to multiple nodes using Dask as we will see through this notebook. \n",
    "\n",
    "For the purposes of this demo, we will use the a part of the NYC Taxi Dataset (only the files of 2014 calendar year will be used here). The goal is to predict the fare amount for a given trip given the times and coordinates of the taxi trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, run the following commands in the terminal to setup Azure CLI\n",
    "```\n",
    "pip install azure-cli\n",
    "az login\n",
    "```\n",
    "And follow the instructions on the prompt to finish setting up the account.\n",
    "\n",
    "The list of packages needed for this notebook is listed in the cell below - uncomment and run the cell to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"dask-cloudprovider[azure]\"\n",
    "# !pip install \"dask-cloudprovider[azure]\" --upgrade\n",
    "# !pip install --upgrade azure-mgmt-network azure-mgmt-compute\n",
    "# !pip install azureml\n",
    "# !pip install azure-storage-blob\n",
    "\n",
    "# # Run the statements below one after the other in order.\n",
    "# !pip install azureml-opendatasets\n",
    "# !pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "import dask_cudf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure cluster set up\n",
    "\n",
    "Let us now setup the [Azure cluster](https://cloudprovider.dask.org/en/latest/azure.html) using `AzureVMCluster` from Dask Cloud Provider. To do this, you;ll first need to set up a Resource Group, a Virtual Network and a Security Group on Azure. [Learn more about how you can set this up](https://cloudprovider.dask.org/en/latest/azure.html#resource-groups). Note that you can also set it up using the Azure portal directly.\n",
    "\n",
    "Once you have set it up, you can now plug in the names of the entities you have created in the cell below. Finally note that we use the RAPIDS docker image to build the VM and use the `dask_cuda.CUDAWorker` to run within the VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"SOUTH CENTRAL US\"\n",
    "resource_group = \"RAPIDS-mnmg\"\n",
    "vnet = \"dask-vnet\"\n",
    "security_group = \"dask-nsg\"\n",
    "\n",
    "vm_size = \"Standard_NC12s_v3\"\n",
    "docker_image = \"rapidsai/rapidsai:0.19-cuda11.0-runtime-ubuntu18.04-py3.8\"\n",
    "worker_class = \"dask_cuda.CUDAWorker\"\n",
    " \n",
    "n_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cloudprovider.azure import AzureVMCluster\n",
    "\n",
    "cluster = AzureVMCluster(\n",
    "    location=location,\n",
    "    resource_group=resource_group,\n",
    "    vnet=vnet,\n",
    "    security_group=security_group,\n",
    "    vm_size=vm_size,\n",
    "    docker_image=docker_image,\n",
    "    worker_class=worker_class,\n",
    "    n_workers=n_workers,\n",
    "    security=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data locally to see what we're dealing with. We will make use of the data from 2014 for the purposes of the demo. We see that there are columns for pickup and dropoff times, distance, along with latitude, longitude, etc. These are the information we'll use to estimate the trip fare amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup\n",
    "\n",
    "The data needs to be cleaned up before it can be used in a meaningful way. We first perform a renaming of some columns to a cleaner name (for instance, some of the years have `tpep_ropoff_datetime` instead of `dropfoff_datetime`). We also define the datatypes each of the columns need to be read as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    " 'tpepPickupDateTime': 'datetime64[ms]',\n",
    " 'tpepDropoffDateTime': 'datetime64[ms]',\n",
    " 'passengerCount': 'int32',\n",
    " 'tripDistance': 'float32',\n",
    " 'startLon': 'float32',\n",
    " 'startLat': 'float32',\n",
    " 'rateCodeId': 'int32',\n",
    " 'endLon': 'float32',\n",
    " 'endLat': 'float32',\n",
    " 'fareAmount': 'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_part, must_haves):\n",
    "    \"\"\"\n",
    "    This function performs the various clean up tasks for the data\n",
    "    and returns the cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['tpepPickupDateTime', 'tpepDropoffDateTime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Interesting Features\n",
    "\n",
    "We'll add new features by making use of \"uder defined functions\" on the dataframe. We'll make use of [apply_rows](https://docs.rapids.ai/api/cudf/stable/api.html#cudf.core.dataframe.DataFrame.apply_rows), which is similar to Pandas' apply funciton. `apply_rows` operation is [JIT compiled by numba](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) into GPU kernels. \n",
    "\n",
    "The kernels we define are - \n",
    "1. Haversine distance: This is used for calculating the total trip distance.\n",
    "\n",
    "2. Day of the week: This can be useful information for determining the fare cost.\n",
    "\n",
    "`add_features` function combined the two to produce a new dataframe that has the added features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "def haversine_distance_kernel(startLat, startLon, endLat, endLon, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(startLat, startLon, endLat, endLon,)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['tpepPickupDateTime'].dt.hour\n",
    "    df['year'] = df['tpepPickupDateTime'].dt.year\n",
    "    df['month'] = df['tpepPickupDateTime'].dt.month\n",
    "    df['day'] = df['tpepPickupDateTime'].dt.day\n",
    "    df['diff'] = (df['tpepDropoffDateTime'] - df['tpepPickupDateTime']).dt.seconds #convert difference between pickup and dropoff into seconds\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['startLat']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['startLon']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['endLat']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['endLon']//.01*.01\n",
    "    \n",
    "    df = df.drop('tpepDropoffDateTime', axis=1)\n",
    "    df = df.drop('tpepPickupDateTime', axis =1)\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                   incols=['startLat', 'startLon', 'endLat', 'endLon'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RF model\n",
    "\n",
    "We are now ready to fit a Random Forest on the data to predict the fare for the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a client with the cluster we defined earlier in the notebook. Note that we have `cluster.scale`. This is the step where the workers are allocated.\n",
    "\n",
    "Once workers become available, we can now run the rest of our workflow - reading and cleaning the data, splitting into training and validation sets, fitting a RF model and predicting on the validation set. We print out the MSE metric for this problem. Note that for better performance we should perform HPO ideally. \n",
    "\n",
    "\n",
    "Refer to the notebooks in the repository for how to perform automated HPO [using RayTune](https://github.com/rapidsai/cloud-ml-examples/blob/main/ray/notebooks/Ray_RAPIDS_HPO.ipynb) and [using Optuna](https://github.com/rapidsai/cloud-ml-examples/blob/main/optuna/notebooks/optuna_rapids.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client(cluster) as client:\n",
    "    print(\"Start Workflow\")\n",
    "    cluster.scale(n_workers)\n",
    "    # Number of GPUS per node for the VM we've spun up is 2\n",
    "    client.wait_for_workers(n_workers * 2)\n",
    "    print(\"Step 0 - Got Workers\")\n",
    "    print(client)\n",
    "\n",
    "    import cudf\n",
    "    # This is a package in preview.\n",
    "    from azureml.opendatasets import NycTlcYellow\n",
    "\n",
    "    from datetime import datetime\n",
    "    from dateutil import parser\n",
    "\n",
    "\n",
    "    end_date = parser.parse('2018-06-06')\n",
    "    start_date = parser.parse('2018-05-01')\n",
    "    nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n",
    "    \n",
    "    df = dask_cudf.from_cudf(cudf.from_pandas(nyc_tlc_df), npartitions=n_workers * 2)\n",
    "\n",
    "    print(\"Step 1 - Finished reading file\")\n",
    "    # Query the dataframe to clean up the outliers \n",
    "\n",
    "    df = clean(df, must_haves)\n",
    "    print(\"Step 2 - Cleaned data\")\n",
    "\n",
    "    taxi_df = df.map_partitions(add_features)\n",
    "    print(\"Step 3 - Added features\")\n",
    "\n",
    "    taxi_df = taxi_df.dropna()\n",
    "    taxi_df = taxi_df.astype(\"float32\")\n",
    "\n",
    "    from dask_ml.model_selection import train_test_split\n",
    "    from cuml.dask.common import utils as dask_utils\n",
    "    X, y = taxi_df.drop([\"fareAmount\"], axis=1), taxi_df[\"fareAmount\"].astype('float32')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
    "    print(\"Step 4 - Split data\")\n",
    "\n",
    "\n",
    "\n",
    "    workers = client.has_what().keys()\n",
    "    X_train, X_test, y_train, y_test = dask_utils.persist_across_workers(client,\n",
    "                                                           [X_train, X_test, y_train, y_test],\n",
    "                                                           workers=workers)\n",
    "\n",
    "    from cuml.dask.ensemble import RandomForestRegressor\n",
    "    cu_dask_rf = RandomForestRegressor(ignore_empty_partitions=True)\n",
    "    cu_dask_rf = cu_dask_rf.fit(X_train, y_train)\n",
    "    wait(cu_dask_rf.rfs)\n",
    "    print(\"Step 5 - Fitted RF\")\n",
    "\n",
    "    y_pred = cu_dask_rf.predict(X_test)\n",
    "    print(\"Step 6 - Predicted on test set\")\n",
    "\n",
    "    _y_pred, _y_test = y_pred.compute().to_array(), y_test.compute().to_array()\n",
    "\n",
    "    print(\"Step 7 - Calculating MSE\")\n",
    "    from cuml.metrics import mean_squared_error\n",
    "    score = mean_squared_error(_y_pred, _y_test)\n",
    "    print(\"Workflow Complete - RMSE: \", np.sqrt(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
